{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (works in Google Colab, Carnets, or iSH)\n",
    "!pip install --upgrade pip\n",
    "!pip install PyPDF2 pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- IMPORT MODULES ----\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from PyPDF2 import PdfReader\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download NLTK stopwords if not already available\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "pdf_path = \"input.pdf\"  # Replace with your PDF path or upload in Colab\n",
    "output_csv_keywords = \"purview_keywords.csv\"\n",
    "output_csv_regex = \"purview_regex.csv\"\n",
    "min_word_length = 3          # Minimum characters for useful words\n",
    "min_phrase_frequency = 2     # Minimum occurrence for multi-word phrases\n",
    "\n",
    "# ---- STOPWORDS ----\n",
    "stopwords_set = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- EXTRACT PDF TEXT ----\n",
    "reader = PdfReader(pdf_path)\n",
    "full_text = \"\"\n",
    "for page in reader.pages:\n",
    "    page_text = page.extract_text()\n",
    "    if page_text:\n",
    "        full_text += page_text + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CLEAN AND TOKENIZE ----\n",
    "text_lower = full_text.lower()\n",
    "text_clean = re.sub(r\"[^\\w\\s]\", \" \", text_lower)  # Remove punctuation\n",
    "tokens = [t for t in text_clean.split() if t not in stopwords_set and len(t) >= min_word_length and not t.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- GENERATE N-GRAMS (multi-word phrases) ----\n",
    "ngram_counts = Counter()\n",
    "for n in range(2, 4):  # 2-grams and 3-grams\n",
    "    for ng in ngrams(tokens, n):\n",
    "        ngram_str = \"_\".join(ng)\n",
    "        ngram_counts[ngram_str] += 1\n",
    "\n",
    "# Keep only frequent n-grams\n",
    "common_phrases = [k for k, v in ngram_counts.items() if v >= min_phrase_frequency]\n",
    "\n",
    "# Replace multi-word phrases in text to treat them as single token\n",
    "processed_text = text_clean\n",
    "for phrase in common_phrases:\n",
    "    processed_text = processed_text.replace(phrase.replace(\"_\", \" \"), phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- GENERATE KEYWORDS ----\n",
    "words = re.findall(r\"\\b[a-zA-Z_]+\\b\", processed_text)\n",
    "words_filtered = [w for w in words if w not in stopwords_set and len(w) >= min_word_length and not w.isdigit()]\n",
    "\n",
    "word_counts = Counter(words_filtered)\n",
    "\n",
    "# Optionally remove overly common English words (can be customized)\n",
    "common_english = {\"document\", \"page\", \"statement\", \"date\"}\n",
    "for word in common_english:\n",
    "    if word in word_counts:\n",
    "        del word_counts[word]\n",
    "\n",
    "df_keywords = pd.DataFrame(word_counts.items(), columns=[\"Keyword\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
    "df_keywords.to_csv(output_csv_keywords, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- GENERATE NUMERIC REGEX PATTERNS ----\n",
    "regex_patterns = set()\n",
    "\n",
    "# Find numeric sequences in document\n",
    "numeric_sequences = re.findall(r\"\\b\\d[\\d\\s\\-]{2,}\\b\", processed_text)\n",
    "for seq in numeric_sequences:\n",
    "    seq_clean = seq.strip()\n",
    "    regex_seq = re.sub(r\"\\d\", r\"\\\\d\", seq_clean)\n",
    "    regex_seq = re.sub(r\"\\s+\", r\"\\\\s+\", regex_seq)\n",
    "    regex_patterns.add(regex_seq)\n",
    "\n",
    "# Add generic regex patterns\n",
    "generic_patterns = [\n",
    "    r\"\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b\",   # Dates\n",
    "    r\"\\b\\d{4}\\s\\d{4}\\s\\d{4}\\b\",       # 12-digit sequences like card numbers\n",
    "    r\"\\b[A-Z]{2,}\\d{4,}\\b\"            # IDs like TX12345\n",
    "]\n",
    "regex_patterns.update(generic_patterns)\n",
    "\n",
    "df_regex = pd.DataFrame({\"RegexPattern\": list(regex_patterns)})\n",
    "df_regex.to_csv(output_csv_regex, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- OUTPUT SUMMARY ----\n",
    "print(\"Keyword list saved to:\", output_csv_keywords)\n",
    "print(\"Regex patterns saved to:\", output_csv_regex)\n",
    "print(\"Top detected multi-word phrases:\", common_phrases[:20])\n",
    "print(\"Top keywords:\", df_keywords.head(20))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}