{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (works in Google Colab, Carnets, or iSH)\n",
    "!pip install --upgrade pip\n",
    "!pip install PyPDF2 pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- IMPORT MODULES ----\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from PyPDF2 import PdfReader\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download NLTK stopwords if not already available\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "pdf_path = \"input.pdf\"  # Replace with your PDF path or upload in Colab\n",
    "output_csv_keywords = \"purview_keywords.csv\"\n",
    "output_csv_regex = \"purview_regex.csv\"\n",
    "min_word_length = 3          # Minimum characters for useful words\n",
    "min_phrase_frequency = 2     # Minimum occurrence for multi-word phrases\n",
    "\n",
    "# ---- STOPWORDS ----\n",
    "stopwords_set = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- EXTRACT PDF TEXT ----\n",
    "reader = PdfReader(pdf_path)\n",
    "full_text = \"\"\n",
    "for page in reader.pages:\n",
    "    page_text = page.extract_text()\n",
    "    if page_text:\n",
    "        full_text += page_text + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CLEAN AND TOKENIZE ----\n",
    "text_lower = full_text.lower()\n",
    "text_clean = re.sub(r\"[^\\w\\s]\", \" \", text_lower)  # Remove punctuation\n",
    "tokens = [t for t in text_clean.split() if t not in stopwords_set and len(t) >= min_word_length and not t.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- GENERATE N-GRAMS (multi-word phrases) ----\n",
    "ngram_counts = Counter()\n",
    "for n in range(2, 4):  # 2-grams and 3-grams\n",
    "    for ng in ngrams(tokens, n):\n",
    "        ngram_str = \" \".join(ng)   # keep natural spacing\n",
    "        ngram_counts[ngram_str] += 1\n",
    "\n",
    "# Keep only frequent n-grams\n",
    "common_phrases = [k for k, v in ngram_counts.items() if v >= min_phrase_frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- DETECT REGEX CANDIDATES ----\n",
    "regex_patterns = set()\n",
    "\n",
    "# Numeric sequences (IDs, account numbers, references)\n",
    "numeric_sequences = re.findall(r\"\\b\\d[\\d\\s\\-]{4,}\\b\", full_text)\n",
    "for seq in numeric_sequences:\n",
    "    seq_clean = seq.strip()\n",
    "    regex_seq = re.sub(r\"\\d\", r\"\\\\d\", seq_clean)\n",
    "    regex_seq = re.sub(r\"\\s+\", r\"\\\\s+\", regex_seq)\n",
    "    regex_patterns.add(regex_seq)\n",
    "\n",
    "# Alphanumeric patterns (codes like INV12345, REF-2023-ABC)\n",
    "alphanumeric_sequences = re.findall(r\"\\b[A-Z]{2,}[A-Z0-9\\-]{2,}\\b\", full_text)\n",
    "for seq in alphanumeric_sequences:\n",
    "    regex_seq = re.sub(r\"[A-Z]\", \"[A-Z]\", seq)\n",
    "    regex_seq = re.sub(r\"[0-9]\", \"\\\\d\", regex_seq)\n",
    "    regex_patterns.add(regex_seq)\n",
    "\n",
    "# Date-like patterns\n",
    "regex_patterns.add(r\"\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- DETECT KEYWORDS ----\n",
    "candidate_keywords = tokens + common_phrases\n",
    "word_counts = Counter(candidate_keywords)\n",
    "\n",
    "# Remove overly generic terms\n",
    "generic_words = {\n",
    "    \"document\", \"page\", \"statement\", \"date\", \"amount\",\n",
    "    \"details\", \"number\", \"total\", \"payment\", \"invoice\"\n",
    "}\n",
    "for gw in generic_words:\n",
    "    if gw in word_counts:\n",
    "        del word_counts[gw]\n",
    "\n",
    "df_keywords = pd.DataFrame(word_counts.items(), columns=[\"Keyword\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
    "df_keywords.to_csv(output_csv_keywords, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- SAVE REGEX ----\n",
    "df_regex = pd.DataFrame({\"RegexPattern\": list(regex_patterns)})\n",
    "df_regex.to_csv(output_csv_regex, index=False)\n",
    "\n",
    "# ---- OUTPUT SUMMARY ----\n",
    "print(\"Keyword list saved to:\", output_csv_keywords)\n",
    "print(\"Regex patterns saved to:\", output_csv_regex)\n",
    "print(\"\\nTop detected multi-word phrases:\")\n",
    "print(common_phrases[:20])\n",
    "print(\"\\nTop keywords:\")\n",
    "print(df_keywords.head(20))\n",
    "print(\"\\nSample regex patterns:\")\n",
    "print(list(regex_patterns)[:10])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
