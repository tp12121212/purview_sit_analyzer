{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install PyPDF2 pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- IMPORT MODULES ----\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from PyPDF2 import PdfReader\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "pdf_path = 'input.pdf'  # Replace with your PDF path or upload in Colab\n",
    "output_csv_keywords = 'purview_keywords.csv'\n",
    "output_csv_regex = 'purview_regex.csv'\n",
    "min_word_length = 3\n",
    "min_phrase_frequency = 2\n",
    "\n",
    "stopwords_set = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- EXTRACT PDF TEXT ----\n",
    "reader = PdfReader(pdf_path)\n",
    "full_text = ''\n",
    "for page in reader.pages:\n",
    "    page_text = page.extract_text()\n",
    "    if page_text:\n",
    "        full_text += page_text + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CLEAN AND TOKENIZE ----\n",
    "text_lower = full_text.lower()\n",
    "text_clean = re.sub(r'[^\\w\\s]', ' ', text_lower)\n",
    "tokens = [t for t in text_clean.split() if t not in stopwords_set and len(t) >= min_word_length and not t.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- GENERATE MULTI-WORD PHRASES ----\n",
    "ngram_counts = Counter()\n",
    "for n in range(2, 4):  # 2-grams and 3-grams\n",
    "    for ng in ngrams(tokens, n):\n",
    "        ngram_str = ' '.join(ng)\n",
    "        ngram_counts[ngram_str] += 1\n",
    "\n",
    "common_phrases = [k for k, v in ngram_counts.items() if v >= min_phrase_frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- DETECT REGEX CANDIDATES ----\n",
    "regex_candidates = []\n",
    "\n",
    "# Numeric sequences (IDs, references)\n",
    "numeric_sequences = re.findall(r'\\b\\d[\\d\\s\\-]{4,}\\b', full_text)\n",
    "for seq in numeric_sequences:\n",
    "    seq_clean = seq.strip()\n",
    "    regex_seq = re.sub(r'\\d', r'\\\\d', seq_clean)\n",
    "    regex_seq = re.sub(r'\\s+', r'\\\\s+', regex_seq)\n",
    "    regex_candidates.append((regex_seq, 'Numeric sequence (ID / reference)', seq_clean))\n",
    "\n",
    "# Alphanumeric patterns\n",
    "alphanumeric_sequences = re.findall(r'\\b[A-Z]{2,}[A-Z0-9\\-]{2,}\\b', full_text)\n",
    "for seq in alphanumeric_sequences:\n",
    "    regex_seq = re.sub(r'[A-Z]', '[A-Z]', seq)\n",
    "    regex_seq = re.sub(r'[0-9]', r'\\\\d', regex_seq)\n",
    "    regex_candidates.append((regex_seq, 'Alphanumeric code (reference / ID)', seq))\n",
    "\n",
    "# Date-like patterns\n",
    "date_sequences = re.findall(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', full_text)\n",
    "for seq in date_sequences:\n",
    "    regex_seq = r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b'\n",
    "    regex_candidates.append((regex_seq, 'Date', seq))\n",
    "\n",
    "# Deduplicate\n",
    "regex_dict = {}\n",
    "for regex, desc, sample in regex_candidates:\n",
    "    if regex not in regex_dict:\n",
    "        regex_dict[regex] = (desc, sample)\n",
    "\n",
    "df_regex = pd.DataFrame(\n",
    "    [(regex, desc, sample) for regex, (desc, sample) in regex_dict.items()],\n",
    "    columns=['RegexPattern','Description','SampleValue']\n",
    ")\n",
    "df_regex.to_csv(output_csv_regex, index=False)\n",
    "print('Regex patterns saved to:', output_csv_regex)\n",
    "df_regex.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- DETECT KEYWORDS INCLUDING MULTI-WORD PHRASES ----\n",
    "candidate_keywords = tokens + common_phrases\n",
    "word_counts = Counter(candidate_keywords)\n",
    "\n",
    "# Remove generic and sensitive terms\n",
    "generic_words = {'document','page','statement','date','amount','details','number','total','payment','invoice','name','address','phone','email'}\n",
    "\n",
    "def is_sensitive(phrase):\n",
    "    # Heuristic: capitalized names\n",
    "    if re.search(r'\\b[A-Z][a-z]+ [A-Z][a-z]+\\b', phrase):\n",
    "        return True\n",
    "    # Heuristic: addresses\n",
    "    if re.search(r'\\b\\d{1,5}\\s+\\w+', phrase):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "filtered_keywords = {}\n",
    "for kw, count in word_counts.items():\n",
    "    if kw.lower() not in generic_words and not is_sensitive(kw):\n",
    "        filtered_keywords[kw] = count\n",
    "\n",
    "df_keywords = pd.DataFrame(filtered_keywords.items(), columns=['Keyword','Count'])\n",
    "df_keywords = df_keywords.sort_values(by='Count', ascending=False)\n",
    "df_keywords.to_csv(output_csv_keywords, index=False)\n",
    "print('Keyword list (including multi-word phrases) saved to:', output_csv_keywords)\n",
    "df_keywords.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}