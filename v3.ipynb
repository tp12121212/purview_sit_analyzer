{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OBhqQU6lIZBe"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade pip\n",
        "!pip install PyPDF2 pandas nltk"
      ],
      "id": "OBhqQU6lIZBe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ChFeNfKIZBf"
      },
      "outputs": [],
      "source": [
        "# ---- IMPORT MODULES ----\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from PyPDF2 import PdfReader\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as nltk_stopwords"
      ],
      "id": "4ChFeNfKIZBf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHFfFdv4IZBg"
      },
      "outputs": [],
      "source": [
        "# ---- CONFIG ----\n",
        "pdf_path = 'input4.pdf'  # Replace with your PDF path or upload in Colab\n",
        "output_csv_keywords = 'purview_keywords.csv'\n",
        "output_csv_regex = 'purview_regex.csv'\n",
        "min_word_length = 3\n",
        "min_phrase_frequency = 2\n",
        "\n",
        "stopwords_set = set(nltk_stopwords.words('english'))"
      ],
      "id": "IHFfFdv4IZBg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CZkk5o-IZBh"
      },
      "outputs": [],
      "source": [
        "# ---- EXTRACT PDF TEXT ----\n",
        "reader = PdfReader(pdf_path)\n",
        "full_text = ''\n",
        "for page in reader.pages:\n",
        "    page_text = page.extract_text()\n",
        "    if page_text:\n",
        "        full_text += page_text + ' '"
      ],
      "id": "7CZkk5o-IZBh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3mzI_H0IZBh"
      },
      "outputs": [],
      "source": [
        "# ---- CLEAN AND TOKENIZE ----\n",
        "text_lower = full_text.lower()\n",
        "text_clean = re.sub(r'[^\\w\\s]', ' ', text_lower)\n",
        "tokens = [t for t in text_clean.split() if t not in stopwords_set and len(t) >= min_word_length and not t.isdigit()]"
      ],
      "id": "a3mzI_H0IZBh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVTekBpfIZBi"
      },
      "outputs": [],
      "source": [
        "# ---- GENERATE MULTI-WORD PHRASES ----\n",
        "ngram_counts = Counter()\n",
        "for n in range(2, 4):  # 2-grams and 3-grams\n",
        "    for ng in ngrams(tokens, n):\n",
        "        ngram_str = ' '.join(ng)\n",
        "        ngram_counts[ngram_str] += 1\n",
        "\n",
        "common_phrases = [k for k, v in ngram_counts.items() if v >= min_phrase_frequency]"
      ],
      "id": "FVTekBpfIZBi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sZ0omvcIZBi"
      },
      "outputs": [],
      "source": [
        "# ---- DETECT REGEX CANDIDATES ----\n",
        "regex_candidates = []\n",
        "# Date-like patterns\n",
        "date_sequences = re.findall(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', full_text)\n",
        "for seq in date_sequences:\n",
        "    regex_seq = r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b'\n",
        "    regex_candidates.append((regex_seq, 'Date', seq))\n",
        "\n",
        "    # Regex_Ranker_CSCAN_AZURE0070_combined_ranker_2\n",
        "Regex_Ranker_CSCAN_AZURE0070_combined_ranker_2 = re.findall(r'(?i)Key|Credential', full_text)\n",
        "for seq in Regex_Ranker_CSCAN_AZURE0070_combined_ranker_2:\n",
        "\tregex_seq = r'(?i)iotHub'\n",
        "\tregex_candidates.append((regex_seq, 'Regex_Ranker_CSCAN_AZURE0070_combined_ranker_2', seq))\n",
        "\n",
        "\n",
        "# Regex_Ranker_CSCAN_AZURE0050\n",
        "Regex_Ranker_CSCAN_AZURE0050 = re.findall(r'(?i)iotHub', full_text)\n",
        "for seq in Regex_Ranker_CSCAN_AZURE0050:\n",
        "\tregex_seq = r'(?i)EndpointSuffix=([a-z0-9\\\\._]{10,50})[;\"\\']'\n",
        "\tregex_candidates.append((regex_seq, 'Regex_Ranker_CSCAN_AZURE0050', seq))\n",
        "\n",
        "\n",
        "# Regex_Ranker_CSCAN_AZURE0070_combined_ranker\n",
        "Regex_Ranker_CSCAN_AZURE0070_combined_ranker = re.findall(r'(?i)EndpointSuffix=([a-z0-9\\\\._]{10,50})[;\"\\']', full_text)\n",
        "for seq in Regex_Ranker_CSCAN_AZURE0070_combined_ranker:\n",
        "\tregex_seq = r'(?i)Endpoint=(https?://[a-z0-9_]{3,50}\\\\.(table|blob|queue|file)\\\\.[a-z0-9\\\\.]{10,50})/?;'\n",
        "\tregex_candidates.append((regex_seq, 'Regex_Ranker_CSCAN_AZURE0070_combined_ranker', seq))\n",
        "\n",
        "\n",
        "# Regex_Ranker_CSCAN_AZURE0070_combined_ranker_2\n",
        "Regex_Ranker_CSCAN_AZURE0070_combined_ranker_2 = re.findall(r'(?i)Endpoint=(https?://[a-z0-9_]{3,50}\\\\.(table|blob|queue|file)\\\\.[a-z0-9\\\\.]{10,50})/?;', full_text)\n",
        "for seq in Regex_Ranker_CSCAN_AZURE0070_combined_ranker_2:\n",
        "\tregex_seq = r'(?i)batch\\\\.azure\\\\.com'\n",
        "\tregex_candidates.append((regex_seq, 'Regex_Ranker_CSCAN_AZURE0070_2', seq))\n",
        "\n",
        "\n",
        "# Regex_Ranker_CSCAN_AZURE0130_AzureBatch\n",
        "Regex_Ranker_CSCAN_AZURE0130_AzureBatch = re.findall(r'(?i)batch\\\\.azure\\\\.com', full_text)\n",
        "for seq in Regex_Ranker_CSCAN_AZURE0130_AzureBatch:\n",
        "\tregex_seq = r'(?i)AccountName=([a-z0-9_]+);'\n",
        "\tregex_candidates.append((regex_seq, 'Regex_Ranker_CSCAN_AZURE0130_AzureBatch', seq))\n",
        "\n",
        "\n",
        "# Regex_Ranker_CSCAN_AZURE0070_combined_ranker_AccountName\n",
        "Regex_Ranker_CSCAN_AZURE0070_combined_ranker_AccountName = re.findall(r'(?i)AccountName=([a-z0-9_]+);', full_text)\n",
        "for seq in Regex_Ranker_CSCAN_AZURE0070_combined_ranker_AccountName:\n",
        "\tregex_seq = r'(?i)AccountEndpoint=(https?://[a-z0-9_\\\\.]+\\\\.documents\\\\.azure\\\\.com(:\\\\d+)?)/?[;\"\\']'\n",
        "\tregex_candidates.append((regex_seq, 'Regex_Ranker_CSCAN_AZURE0070_combined_ranker_AccountName', seq))\n",
        "\n",
        "\n",
        "# Regex_Ranker_CSCAN_AZURE0080_AccountEndpoint_38496415\n",
        "Regex_Ranker_CSCAN_AZURE0080_AccountEndpoint_38496415 = re.findall(r'(?i)AccountEndpoint=(https?://[a-z0-9_\\\\.]+\\\\.documents\\\\.azure\\\\.com(:\\\\d+)?)/?[;\"\\']', full_text)\n",
        "for seq in Regex_Ranker_CSCAN_AZURE0080_AccountEndpoint_38496415:\n",
        "\tregex_seq = r'(?i)Account|Storage|Access|Primary[^v]|Secondary[^v]|Blob'\n",
        "\tregex_candidates.append((regex_seq, 'Regex_Ranker_CSCAN_AZURE0080_AccountEndpoint_38496415', seq))\n",
        "\n",
        "\n",
        "# Regex_Ranker_CSCAN_AZURE0070_combined_ranker_3\n",
        "Regex_Ranker_CSCAN_AZURE0070_combined_ranker_3 = re.findall(r'(?i)Account|Storage|Access|Primary[^v]|Secondary[^v]|Blob', full_text)\n",
        "for seq in Regex_Ranker_CSCAN_AZURE0070_combined_ranker_3:\n",
        "\tregex_seq = r'(?i)^\\\\Wcore\\\\.windows\\\\.net'\n",
        "\tregex_candidates.append((regex_seq, 'Regex_Ranker_CSCAN_AZURE0070_combined_ranker_3', seq))\n",
        "\n",
        "\n",
        "# Regex_Ranker_CSCAN_AZURE0070_combined_ranker_2\n",
        "Regex_Ranker_CSCAN_AZURE0070_combined_ranker_2 = re.findall(r'(?i)^\\\\Wcore\\\\.windows\\\\.net', full_text)\n",
        "for seq in Regex_Ranker_CSCAN_AZURE0070_combined_ranker_2:\n",
        "\tregex_seq = r'(?i)\\\\Wrefresh.?token'\n",
        "\tregex_candidates.append((regex_seq, 'Regex_Ranker_CSCAN_AZURE0070_combined_ranker_2', seq))\n",
        "\n",
        "\n",
        "# Deduplicate\n",
        "regex_dict = {}\n",
        "for regex, desc, sample in regex_candidates:\n",
        "    if regex not in regex_dict:\n",
        "        regex_dict[regex] = (desc, sample)\n",
        "\n",
        "df_regex = pd.DataFrame(\n",
        "    [(regex, desc, sample) for regex, (desc, sample) in regex_dict.items()],\n",
        "    columns=['RegexPattern','Description','SampleValue']\n",
        ")\n",
        "df_regex.to_csv(output_csv_regex, index=False)\n",
        "print('Regex patterns saved to:', output_csv_regex)\n",
        "df_regex.head(10)"
      ],
      "id": "5sZ0omvcIZBi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J3HebB0IZBj"
      },
      "outputs": [],
      "source": [
        "# ---- DETECT KEYWORDS INCLUDING MULTI-WORD PHRASES ----\n",
        "candidate_keywords = tokens + common_phrases\n",
        "word_counts = Counter(candidate_keywords)\n",
        "\n",
        "# Remove generic and sensitive terms\n",
        "generic_words = {'document','page','statement','date','amount','details','number','total','payment','invoice','name','address','phone','email'}\n",
        "\n",
        "def is_sensitive(phrase):\n",
        "    # Heuristic: capitalized names\n",
        "    if re.search(r'\\b[A-Z][a-z]+ [A-Z][a-z]+\\b', phrase):\n",
        "        return True\n",
        "    # Heuristic: addresses\n",
        "    if re.search(r'\\b\\d{1,5}\\s+\\w+', phrase):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "filtered_keywords = {}\n",
        "for kw, count in word_counts.items():\n",
        "    if kw.lower() not in generic_words and not is_sensitive(kw):\n",
        "        filtered_keywords[kw] = count\n",
        "\n",
        "df_keywords = pd.DataFrame(filtered_keywords.items(), columns=['Keyword','Count'])\n",
        "df_keywords = df_keywords.sort_values(by='Count', ascending=False)\n",
        "df_keywords.to_csv(output_csv_keywords, index=False)\n",
        "print('Keyword list (including multi-word phrases) saved to:', output_csv_keywords)\n",
        "df_keywords.head(20)"
      ],
      "id": "_J3HebB0IZBj"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}